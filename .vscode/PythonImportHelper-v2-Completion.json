[
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "PredictRequest",
        "importPath": "src.api.pydantic_models",
        "description": "src.api.pydantic_models",
        "isExtraImport": true,
        "detail": "src.api.pydantic_models",
        "documentation": {}
    },
    {
        "label": "PredictResponse",
        "importPath": "src.api.pydantic_models",
        "description": "src.api.pydantic_models",
        "isExtraImport": true,
        "detail": "src.api.pydantic_models",
        "documentation": {}
    },
    {
        "label": "mlflow.pyfunc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mlflow.pyfunc",
        "description": "mlflow.pyfunc",
        "detail": "mlflow.pyfunc",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "ColumnTransformer",
        "importPath": "sklearn.compose",
        "description": "sklearn.compose",
        "isExtraImport": true,
        "detail": "sklearn.compose",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "LabelEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "SimpleImputer",
        "importPath": "sklearn.impute",
        "description": "sklearn.impute",
        "isExtraImport": true,
        "detail": "sklearn.impute",
        "documentation": {}
    },
    {
        "label": "BaseEstimator",
        "importPath": "sklearn.base",
        "description": "sklearn.base",
        "isExtraImport": true,
        "detail": "sklearn.base",
        "documentation": {}
    },
    {
        "label": "TransformerMixin",
        "importPath": "sklearn.base",
        "description": "sklearn.base",
        "isExtraImport": true,
        "detail": "sklearn.base",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "RandomForestClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mlflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mlflow",
        "description": "mlflow",
        "detail": "mlflow",
        "documentation": {}
    },
    {
        "label": "mlflow.sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mlflow.sklearn",
        "description": "mlflow.sklearn",
        "detail": "mlflow.sklearn",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "process_data",
        "importPath": "src.data_processing",
        "description": "src.data_processing",
        "isExtraImport": true,
        "detail": "src.data_processing",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "src.api.main",
        "description": "src.api.main",
        "peekOfCode": "def load_model():\n    global model\n    model = mlflow.sklearn.load_model(MODEL_PATH)\n@app.post(\"/predict\", response_model=PredictResponse)\ndef predict(request: PredictRequest):\n    # Convert request to DataFrame\n    input_data = pd.DataFrame([request.dict()])\n    # Predict risk probability (assume binary classification, get proba for class 1)\n    proba = model.predict_proba(input_data)[0][1]\n    return PredictResponse(risk_probability=float(proba))",
        "detail": "src.api.main",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "src.api.main",
        "description": "src.api.main",
        "peekOfCode": "def predict(request: PredictRequest):\n    # Convert request to DataFrame\n    input_data = pd.DataFrame([request.dict()])\n    # Predict risk probability (assume binary classification, get proba for class 1)\n    proba = model.predict_proba(input_data)[0][1]\n    return PredictResponse(risk_probability=float(proba))",
        "detail": "src.api.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "src.api.main",
        "description": "src.api.main",
        "peekOfCode": "app = FastAPI()\n# Load the best model from MLflow registry (for now, load from local path)\nMODEL_PATH = \"mlruns/288870226930422180/models/m-9e119381963b44f89e3eaa6c429cb0c2/artifacts/\"\nmodel = None\n@app.on_event(\"startup\")\ndef load_model():\n    global model\n    model = mlflow.sklearn.load_model(MODEL_PATH)\n@app.post(\"/predict\", response_model=PredictResponse)\ndef predict(request: PredictRequest):",
        "detail": "src.api.main",
        "documentation": {}
    },
    {
        "label": "MODEL_PATH",
        "kind": 5,
        "importPath": "src.api.main",
        "description": "src.api.main",
        "peekOfCode": "MODEL_PATH = \"mlruns/288870226930422180/models/m-9e119381963b44f89e3eaa6c429cb0c2/artifacts/\"\nmodel = None\n@app.on_event(\"startup\")\ndef load_model():\n    global model\n    model = mlflow.sklearn.load_model(MODEL_PATH)\n@app.post(\"/predict\", response_model=PredictResponse)\ndef predict(request: PredictRequest):\n    # Convert request to DataFrame\n    input_data = pd.DataFrame([request.dict()])",
        "detail": "src.api.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "src.api.main",
        "description": "src.api.main",
        "peekOfCode": "model = None\n@app.on_event(\"startup\")\ndef load_model():\n    global model\n    model = mlflow.sklearn.load_model(MODEL_PATH)\n@app.post(\"/predict\", response_model=PredictResponse)\ndef predict(request: PredictRequest):\n    # Convert request to DataFrame\n    input_data = pd.DataFrame([request.dict()])\n    # Predict risk probability (assume binary classification, get proba for class 1)",
        "detail": "src.api.main",
        "documentation": {}
    },
    {
        "label": "PredictRequest",
        "kind": 6,
        "importPath": "src.api.pydantic_models",
        "description": "src.api.pydantic_models",
        "peekOfCode": "class PredictRequest(BaseModel):\n    ProviderId_ProviderId_1: float\n    ProviderId_ProviderId_2: float\n    ProviderId_ProviderId_3: float\n    ProviderId_ProviderId_4: float\n    ProviderId_ProviderId_5: float\n    ProviderId_ProviderId_6: float\n    ProductCategory_airtime: float\n    ProductCategory_data_bundles: float\n    ProductCategory_financial_services: float",
        "detail": "src.api.pydantic_models",
        "documentation": {}
    },
    {
        "label": "PredictResponse",
        "kind": 6,
        "importPath": "src.api.pydantic_models",
        "description": "src.api.pydantic_models",
        "peekOfCode": "class PredictResponse(BaseModel):\n    risk_probability: float",
        "detail": "src.api.pydantic_models",
        "documentation": {}
    },
    {
        "label": "AggregateCustomerFeatures",
        "kind": 6,
        "importPath": "src.data_processing",
        "description": "src.data_processing",
        "peekOfCode": "class AggregateCustomerFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to create aggregate features per customer and merge them back to the transaction level.\n    \"\"\"\n    def __init__(self, customer_id_col='CustomerId', amount_col='Amount'):\n        self.customer_id_col = customer_id_col\n        self.amount_col = amount_col\n        self.agg_features_ = None\n    def fit(self, X, y=None):\n        return self",
        "detail": "src.data_processing",
        "documentation": {}
    },
    {
        "label": "DateTimeFeatureExtractor",
        "kind": 6,
        "importPath": "src.data_processing",
        "description": "src.data_processing",
        "peekOfCode": "class DateTimeFeatureExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to extract date/time features from TransactionStartTime.\n    \"\"\"\n    def __init__(self, datetime_col='TransactionStartTime'):\n        self.datetime_col = datetime_col\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()",
        "detail": "src.data_processing",
        "documentation": {}
    },
    {
        "label": "WOEFeatureTransformer",
        "kind": 6,
        "importPath": "src.data_processing",
        "description": "src.data_processing",
        "peekOfCode": "class WOEFeatureTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to apply WOE encoding to categorical variables using xverse.\n    \"\"\"\n    def __init__(self, categorical_cols, target_col='FraudResult'):\n        self.categorical_cols = categorical_cols\n        self.target_col = target_col\n        self.woe = None\n        self.woe_cols = None\n    def fit(self, X, y=None):",
        "detail": "src.data_processing",
        "documentation": {}
    },
    {
        "label": "build_preprocessing_pipeline",
        "kind": 2,
        "importPath": "src.data_processing",
        "description": "src.data_processing",
        "peekOfCode": "def build_preprocessing_pipeline():\n    # Define columns\n    categorical_cols = ['ProviderId', 'ProductCategory', 'ChannelId', 'ProductId']\n    numerical_cols = ['Amount', 'Value', 'customer_total_amount', 'customer_avg_amount',\n                     'customer_transaction_count', 'customer_std_amount',\n                     'transaction_hour', 'transaction_day', 'transaction_month', 'transaction_year']\n    # Pipelines for different column types\n    categorical_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))",
        "detail": "src.data_processing",
        "documentation": {}
    },
    {
        "label": "process_data",
        "kind": 2,
        "importPath": "src.data_processing",
        "description": "src.data_processing",
        "peekOfCode": "def process_data(input_path):\n    df = pd.read_csv(input_path)\n    pipeline = build_preprocessing_pipeline()\n    processed = pipeline.fit_transform(df)\n    return processed\nif __name__ == \"__main__\":\n    processed = process_data(\"data/raw/data.csv\")\n    print(processed)",
        "detail": "src.data_processing",
        "documentation": {}
    },
    {
        "label": "processed_df",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "processed_df = pd.read_csv('data/processed/processed_data.csv')\n# Load RFM DataFrame with is_high_risk label\nrfm = pd.read_csv('data/processed/rfm_with_risk.csv')\n# Merge is_high_risk into processed features\nmerged_df = processed_df.merge(rfm[['CustomerId', 'is_high_risk']], on='CustomerId', how='left')\n# Save merged data for modeling\nmerged_df.to_csv('data/processed/model_ready_data.csv', index=False)\n# Prepare features and target\ny = merged_df['is_high_risk']\nX = merged_df.drop(['CustomerId', 'is_high_risk'], axis=1)",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "rfm",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "rfm = pd.read_csv('data/processed/rfm_with_risk.csv')\n# Merge is_high_risk into processed features\nmerged_df = processed_df.merge(rfm[['CustomerId', 'is_high_risk']], on='CustomerId', how='left')\n# Save merged data for modeling\nmerged_df.to_csv('data/processed/model_ready_data.csv', index=False)\n# Prepare features and target\ny = merged_df['is_high_risk']\nX = merged_df.drop(['CustomerId', 'is_high_risk'], axis=1)\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "merged_df",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "merged_df = processed_df.merge(rfm[['CustomerId', 'is_high_risk']], on='CustomerId', how='left')\n# Save merged data for modeling\nmerged_df.to_csv('data/processed/model_ready_data.csv', index=False)\n# Prepare features and target\ny = merged_df['is_high_risk']\nX = merged_df.drop(['CustomerId', 'is_high_risk'], axis=1)\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "y = merged_df['is_high_risk']\nX = merged_df.drop(['CustomerId', 'is_high_risk'], axis=1)\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n# Define models and parameter grids\nmodels = {\n    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"RandomForest\": RandomForestClassifier(random_state=42)",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "X = merged_df.drop(['CustomerId', 'is_high_risk'], axis=1)\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n# Define models and parameter grids\nmodels = {\n    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"RandomForest\": RandomForestClassifier(random_state=42)\n}",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "models = {\n    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"RandomForest\": RandomForestClassifier(random_state=42)\n}\nparam_grids = {\n    \"LogisticRegression\": {\"C\": [0.1, 1, 10]},\n    \"RandomForest\": {\"n_estimators\": [100, 200], \"max_depth\": [None, 10, 20]}\n}\nbest_estimators = {}\nmlflow.set_experiment(\"credit_scoring\")",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "param_grids",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "param_grids = {\n    \"LogisticRegression\": {\"C\": [0.1, 1, 10]},\n    \"RandomForest\": {\"n_estimators\": [100, 200], \"max_depth\": [None, 10, 20]}\n}\nbest_estimators = {}\nmlflow.set_experiment(\"credit_scoring\")\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    grid = GridSearchCV(model, param_grids[name], cv=3, scoring='f1', n_jobs=-1)\n    grid.fit(X_train, y_train)",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "best_estimators",
        "kind": 5,
        "importPath": "src.train_model",
        "description": "src.train_model",
        "peekOfCode": "best_estimators = {}\nmlflow.set_experiment(\"credit_scoring\")\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    grid = GridSearchCV(model, param_grids[name], cv=3, scoring='f1', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    best_estimators[name] = grid.best_estimator_\n    print(f\"{name} best params: {grid.best_params_}\")\n    # Evaluate\n    y_pred = grid.predict(X_test)",
        "detail": "src.train_model",
        "documentation": {}
    },
    {
        "label": "input_path",
        "kind": 5,
        "importPath": "run_data_processing",
        "description": "run_data_processing",
        "peekOfCode": "input_path = 'data/raw/data.csv'\noutput_path = 'data/processed/processed_data.csv'\n# Run the processing pipeline\nprocessed = process_data(input_path)\n# If processed is a numpy array, convert to DataFrame\nif not isinstance(processed, pd.DataFrame):\n    processed = pd.DataFrame(processed)\nprocessed.to_csv(output_path, index=False)\nprint(f'Processed data saved to {output_path}')",
        "detail": "run_data_processing",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "run_data_processing",
        "description": "run_data_processing",
        "peekOfCode": "output_path = 'data/processed/processed_data.csv'\n# Run the processing pipeline\nprocessed = process_data(input_path)\n# If processed is a numpy array, convert to DataFrame\nif not isinstance(processed, pd.DataFrame):\n    processed = pd.DataFrame(processed)\nprocessed.to_csv(output_path, index=False)\nprint(f'Processed data saved to {output_path}')",
        "detail": "run_data_processing",
        "documentation": {}
    },
    {
        "label": "processed",
        "kind": 5,
        "importPath": "run_data_processing",
        "description": "run_data_processing",
        "peekOfCode": "processed = process_data(input_path)\n# If processed is a numpy array, convert to DataFrame\nif not isinstance(processed, pd.DataFrame):\n    processed = pd.DataFrame(processed)\nprocessed.to_csv(output_path, index=False)\nprint(f'Processed data saved to {output_path}')",
        "detail": "run_data_processing",
        "documentation": {}
    }
]